import streamlit as st
from langchain_ollama import OllamaEmbeddings, OllamaLLM
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
# from langchain_community.embeddings import HuggingFaceBgeEmbeddings
# import os 
# os.environ["OLLAMA_NUM_GPU"] = "0"

st.title("ğŸ¤– è¼•é‡ç´š PDF åŠ©æ‰‹ (Qwen 0.5B)")

@st.cache_resource
def init_rag():
    # 1. å‘é‡åº«èˆ‡ Embedding
    #model_name = "BAAI/bge-small-zh-v1.5"
    #embeddings = HuggingFaceBgeEmbeddings(
    #    model_name=model_name,
    #    model_kwargs={'device': 'cpu'} # é¡¯å¼æŒ‡å®š CPU
    #)
    embeddings = OllamaEmbeddings(model="nomic-embed-text")
    # embeddings = OllamaEmbeddings(model="bge-m3",num_gpu=0)
    
    vector_db = Chroma(persist_directory="./db", embedding_function=embeddings)
    retriever = vector_db.as_retriever(search_kwargs={"k": 2})

    # 2. æ¨¡å‹
    # llm = OllamaLLM(model="qwen2.5:0.5b",temperature=0,num_ctx=2048)
    # llm = OllamaLLM(model="qwen2.5:1.5b",temperature=0,num_ctx=2048)
    # llm = OllamaLLM(model="qwen3:0.6b",temperature=0,num_ctx=2048)
    llm = OllamaLLM(model="qwen3:1.7b")

    # llm = OllamaLLM(model="deepseek-coder:1.3b",temperature=0,num_ctx=2048)
    # llm = OllamaLLM(model="deepseek-r1:1.5b")

    # 3. å»ºç«‹ Prompt æ¨¡æ¿ (è®“ AI æ ¹æ“šè³‡æ–™å›ç­”)
    template = """ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„åŠ©æ‰‹ã€‚è«‹æ ¹æ“šä»¥ä¸‹æä¾›çš„è³‡æ–™ä¾†å›ç­”å•é¡Œã€‚
    å¦‚æœè³‡æ–™ä¸­æ²’æœ‰ç›¸é—œå…§å®¹ï¼Œè«‹èª å¯¦å›ç­”ã€Œæˆ‘ä¸çŸ¥é“ã€ï¼Œä¸è¦èƒ¡æ‰¯ã€‚

    è³‡æ–™å…§å®¹:
    {context}

    å•é¡Œ: {question}
    """
    prompt = ChatPromptTemplate.from_template(template)

    # 4. å»ºç«‹ RAG éˆ (LCEL å¯«æ³•ï¼Œå–ä»£èˆŠçš„ RetrievalQA)
    rag_chain = (
        {"context": retriever, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )
    return rag_chain

bot = init_rag()

# èŠå¤©ä»‹é¢
if user_input := st.chat_input("å•æˆ‘é—œæ–¼ PDF çš„å•é¡Œ"):
    st.chat_message("user").write(user_input)
    with st.spinner("AI æ­£åœ¨æ€è€ƒ..."):
        # å‘¼å«æ–°ç‰ˆçš„ Chain
        response = bot.invoke(user_input)
        st.chat_message("assistant").write(response)