import os
import json
from langchain_community.document_loaders import Docx2txtLoader # æ›æˆé€™å€‹
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceBgeEmbeddings
from langchain_chroma import Chroma

# --- è¨­å®šèˆ‡ä¹‹å‰ç›¸åŒ ---
DATA_DIR = "./data_docx"
DB_DIR = "./db_bge_word"  # å»ºè­°æ›å€‹åå­—å€åˆ†
RECORD_FILE = "processed_files_word.json"

# åˆå§‹åŒ– BGE (CPU)
model_name = "BAAI/bge-small-zh-v1.5"
embeddings = HuggingFaceBgeEmbeddings(
    model_name=model_name,
    model_kwargs={'device': 'cpu'},
    encode_kwargs={'normalize_embeddings': True}
)

def main():
    if os.path.exists(RECORD_FILE):
        with open(RECORD_FILE, "r") as f:
            processed_files = set(json.load(f))
    else:
        processed_files = set()

    # ä¿®æ”¹ç‚ºæƒæ .docx æª”æ¡ˆ
    all_files = [f for f in os.listdir(DATA_DIR) if f.endswith(".docx")]
    new_files = [f for f in all_files if f not in processed_files]

    if not new_files:
        print("âœ… æ‰€æœ‰ Word æª”æ¡ˆçš†å·²ç´¢å¼•ã€‚")
        return

    # Word æª”æ¡ˆé€šå¸¸æ®µè½è¼ƒé•·ï¼Œå»ºè­° chunk_size å¯ä»¥ç¨å¾®èª¿å¤§ä¸€é»
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=150)
    
    for i, file_name in enumerate(new_files):
        print(f"[{i+1}/{len(new_files)}] è™•ç† Word ä¸­: {file_name}")
        try:
            # 1. ä½¿ç”¨ Word è¼‰å…¥å™¨
            loader = Docx2txtLoader(os.path.join(DATA_DIR, file_name))
            docs = loader.load()
            
            # 2. åˆ‡åˆ†æ–‡å­—
            chunks = text_splitter.split_documents(docs)
            
            # 3. å­˜å…¥ ChromaDB
            if not os.path.exists(DB_DIR):
                vector_db = Chroma.from_documents(
                    documents=chunks, embedding=embeddings, persist_directory=DB_DIR
                )
            else:
                vector_db = Chroma(persist_directory=DB_DIR, embedding_function=embeddings)
                vector_db.add_documents(chunks)

            processed_files.add(file_name)
            with open(RECORD_FILE, "w") as f:
                json.dump(list(processed_files), f)
                
        except Exception as e:
            print(f"âŒ æª”æ¡ˆ {file_name} ç™¼ç”ŸéŒ¯èª¤: {e}")

    print(f"ğŸ‰ Word ç´¢å¼•å»ºç½®å®Œæˆï¼è·¯å¾‘: {DB_DIR}")

if __name__ == "__main__":
    main()