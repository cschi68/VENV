import streamlit as st
from langchain_ollama import OllamaEmbeddings, OllamaLLM
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableParallel, RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_community.embeddings import HuggingFaceBgeEmbeddings
import os 
# os.environ["OLLAMA_NUM_GPU"] = "0"


    # ç”¨æ–¼æ ¼å¼åŒ–æª¢ç´¢åˆ°çš„æ–‡ä»¶å…§å®¹ï¼ˆå°‡å¤šå€‹æ–‡ä»¶ç‰‡æ®µåˆä½µæˆä¸€æ®µæ–‡å­—ä¾› LLM è®€å–ï¼‰
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

st.title("ğŸ¤– è¼•é‡ç´šåŠ©æ‰‹ (qwen3:1.7b)")

@st.cache_resource
def init_rag():
    # 1. å‘é‡åº«èˆ‡ Embedding
    model_name = "BAAI/bge-small-zh-v1.5"
    embeddings = HuggingFaceBgeEmbeddings(
        model_name=model_name,
        model_kwargs={'device': 'cpu'} # é¡¯å¼æŒ‡å®š CPU
    )
    # embeddings = OllamaEmbeddings(model="nomic-embed-text")
    # embeddings = OllamaEmbeddings(model="bge-m3",num_gpu=0)
    
    vector_db = Chroma(persist_directory="./db_bge_word", embedding_function=embeddings)
    retriever = vector_db.as_retriever(search_kwargs={"k": 4})
    # 2. æ¨¡å‹
    # llm = OllamaLLM(model="qwen2.5:0.5b",temperature=0,num_ctx=2048)
    # llm = OllamaLLM(model="qwen2.5:1.5b",temperature=0,num_ctx=2048)
    # llm = OllamaLLM(model="qwen3:0.6b",temperature=0,num_ctx=2048)
    llm = OllamaLLM(model="qwen3:1.7b")

    # llm = OllamaLLM(model="deepseek-coder:1.3b",temperature=0,num_ctx=2048)
    # llm = OllamaLLM(model="deepseek-r1:1.5b")

    # 3. å»ºç«‹ Prompt æ¨¡æ¿ (è®“ AI æ ¹æ“šè³‡æ–™å›ç­”)
    template = """
    ä½ ç¾åœ¨æ˜¯ã€æ ¸èƒ½é›»å» å®‰å…¨é‹ä½œèˆ‡ç¨‹åºè¦ç¯„å°ˆå®¶ã€‘ã€‚
    ä½ çš„ä»»å‹™æ˜¯æ ¹æ“šæä¾›çš„ã€Œç¨‹åºæ›¸ç‰‡æ®µã€å›ç­”ä½¿ç”¨è€…çš„å•é¡Œï¼Œå¦‚æœè³‡æ–™ä¸­æ²’æœ‰ç›¸é—œå…§å®¹ï¼Œè«‹èª å¯¦å›ç­”ã€Œæˆ‘ä¸çŸ¥é“ã€ï¼Œä¸è¦èƒ¡æ‰¯ã€‚

    ### åŸ·è¡Œæº–å‰‡ï¼š
    1. **åš´è¬¹æ€§**ï¼šæ ¸èƒ½å®‰å…¨è‡³ä¸Šã€‚å¦‚æœç¨‹åºæ›¸ç‰‡æ®µä¸­æ²’æœ‰æ˜ç¢ºç­”æ¡ˆï¼Œè«‹å›ç­”ï¼šã€Œæ ¹æ“šç›®å‰è¼‰å…¥çš„ç¨‹åºæ›¸è³‡æ–™ï¼Œç„¡æ³•æä¾›æ­¤å•é¡Œçš„ç¢ºåˆ‡ç­”æ¡ˆï¼Œè«‹æŸ¥é–±åŸå§‹ç´™æœ¬æ–‡ä»¶æˆ–è©¢å•ä¸»ç®¡ã€‚ã€ï¼Œçµ•å°ä¸å¯ç·¨é€ æ•¸å€¼ã€‚
    2. **è­‰æ“šå°å‘**ï¼šåœ¨å›ç­”æ™‚ï¼Œå¦‚æœè³‡æ–™ä¸­æœ‰æåˆ°æª”æ¡ˆåç¨±æˆ–é ç¢¼ï¼Œè«‹å‹™å¿…æ¨™è¨»ï¼ˆä¾‹å¦‚ï¼šæ ¹æ“š [ç¨‹åºæ›¸A-01] ç¬¬12é ...ï¼‰ã€‚
    3. **å°ˆæ¥­è¡“èª**ï¼šè«‹ä½¿ç”¨å°ˆæ¥­çš„ä¸­æ–‡åŒ–æŠ€è¡“è¡“èªï¼Œä¸è¦å£èªåŒ–ã€‚
    4. **çµæ§‹åŒ–å›è¦†**ï¼šå¦‚æœç­”æ¡ˆåŒ…å«æ­¥é©Ÿï¼Œè«‹ä½¿ç”¨ 1. 2. 3. æ¨™è™Ÿåˆ—å‡ºã€‚

    ### åƒè€ƒè³‡æ–™ï¼ˆContextï¼‰ï¼š
    è³‡æ–™å…§å®¹:
    {context}

    ### ä½¿ç”¨è€…å•é¡Œï¼š
    å•é¡Œ: {question}    
    """    
    prompt = ChatPromptTemplate.from_template(template)

    # 3. å»ºç«‹ LCEL RAG éˆ
    # æˆ‘å€‘æŠŠ retrieval åˆ†é–‹ï¼Œä»¥ä¾¿æœ€å¾Œèƒ½æ‹¿åˆ°åŸå§‹çš„ docs
    # å»ºç«‹ä¸€å€‹ç¨ç«‹çš„è™•ç†éˆ
    # ç¬¬ä¸€æ­¥ï¼šç²å–æ–‡æª”èˆ‡ä¿ç•™å•é¡Œ
    map_chain = RunnableParallel({
        "context_docs": lambda x: retriever.invoke(x["question"]),
        "question": lambda x: x["question"]
    })

    # ç¬¬äºŒæ­¥ï¼šçµ„åˆæœ€çµ‚è¼¸å‡º
    # ä½¿ç”¨ assign é€æ­¥å¢åŠ æ¬„ä½ï¼Œç¢ºä¿è³‡æ–™å§‹çµ‚ä»¥ dict æ ¼å¼å‚³é
    full_chain = (
        map_chain 
        | RunnablePassthrough.assign(
            context=lambda x: format_docs(x["context_docs"])
        )
        | RunnablePassthrough.assign(
            answer=prompt | llm | StrOutputParser()
        )
    )
    
    return full_chain

bot = init_rag()

# 1. åˆå§‹åŒ–å°è©±ç´€éŒ„ (å¦‚æœé‚„ä¸å­˜åœ¨çš„è©±)
if "messages" not in st.session_state:
    st.session_state.messages = []

# 2. é¡¯ç¤ºéå»çš„å°è©±ç´€éŒ„
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        
# èŠå¤©ä»‹é¢
if user_input := st.chat_input("å•æˆ‘é—œæ–¼ æ ¸èƒ½é›»å» ç·Šæ€¥æ‡‰è®Šç¨‹åº çš„å•é¡Œ"):
    
    # æŠŠä½¿ç”¨è€…çš„è©±å­˜å…¥è¨˜æ†¶
    st.session_state.messages.append({"role": "user", "content": user_input})
    
    st.chat_message("user").write(user_input)
    with st.spinner("AI æ­£åœ¨æ€è€ƒ..."):
        # é—œéµï¼šé€™è£¡å¿…é ˆå‚³å…¥ dict
        response = bot.invoke({"question": user_input})
        
        # å–å¾—å›ç­”
        answer = response.get("answer", "ç„¡æ³•ç”Ÿæˆå›ç­”")
        sources = response.get("context_docs", [])
        
        #
        # --- é—œéµä¿®æ­£é»ï¼šåªæå– answer å­—ä¸² ---
        full_answer = response["answer"]
        # sources = response["context"] # é€™æ˜¯åŸå§‹çš„ Documents åˆ—è¡¨
        
        with st.chat_message("assistant"):
            st.markdown(answer)
            
            # é¡¯ç¤ºä¾†æº
            if sources:
                with st.expander("ğŸ“š æŸ¥çœ‹åŸå§‹ç¨‹åºæ›¸ä¾†æº"):
                    for doc in sources:
                        src = os.path.basename(doc.metadata.get('source', 'æœªçŸ¥'))
                        st.write(f"ğŸ“„ {src}")
                        st.caption(doc.page_content[:200] + "...")
                        
        # å°‡ AI çš„å›ç­”å­˜å…¥ session (åªå­˜å­—ä¸²ï¼Œä¸å­˜æ•´å€‹ response ç‰©ä»¶)
        st.session_state.messages.append({"role": "assistant", "content": full_answer})